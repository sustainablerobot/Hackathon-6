# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MVlSesSzpA_TM0KO7r-SduVCHuAWEG2c
"""

# This is the single, complete installation command for all features.
!pip install -q --upgrade langchain langchain-community langchain-google-genai google-generativeai faiss-cpu "unstructured[local-inference]" python-magic python-docx langchain-unstructured flask pyngrok flask-cors
!apt-get -qq install libmagic1


import os
import json
import re
import shutil
import glob
import threading
from google.colab import userdata, files
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_unstructured import UnstructuredLoader # The new, correct loader
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from flask import Flask, request, jsonify
from pyngrok import ngrok
from flask_cors import CORS


try:
    os.environ["GOOGLE_API_KEY"] = userdata.get('GOOGLE_API_KEY')
    print("API Key loaded successfully.")
except userdata.SecretNotFoundError:
    print("ERROR: Secret 'GOOGLE_API_KEY' not found. Please add it to your Colab secrets.")

MODEL_CONFIG = {
    "llm": "gemini-1.5-flash-latest",
    "embedding": "models/embedding-001"
}



def create_vector_store_from_paths_universal(file_paths):

    all_documents = []
    try:
        for file_path in file_paths:
            print(f"-> Loading {file_path}...")
            loader = UnstructuredLoader(file_path, mode="elements")
            all_documents.extend(loader.load())

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        docs = text_splitter.split_documents(all_documents)

        embeddings = GoogleGenerativeAIEmbeddings(model=MODEL_CONFIG["embedding"])
        vector_store = FAISS.from_documents(docs, embeddings)
        print(f"\nVector store created successfully from {len(file_paths)} file(s) with {len(docs)} document chunks.")
        return vector_store
    except Exception as e:
        print(f"Error creating vector store: {e}")
        return None

def get_query_topic_with_llm(user_query, llm):
    """
    Stage 1: The 'Router' LLM. Identifies the core insurance topic from the user's query.
    """


    topic_list = [
        "Cataract Surgery Sub-Limit",
        "Maternity Expenses Waiting Period",
        "Maternity Coverage Amount",
        "AYUSH Treatment Coverage",
        "Adventure Sports Exclusion",
        "Hernia Waiting Period",
        "Domiciliary Hospitalization Exclusion",
        "Grace Period Policy",
        "Dental Treatment Accident",
        "Robotic Surgery Sub-Limit",
        "Post-Hospitalization Time Limit",
        "General Inquiry"
        "Knee Surgery"
    ]




    router_prompt_template = """
    You are an intelligent router AI. Your task is to analyze the user's query and identify the single most relevant insurance topic from the provided list.
    Return only the topic name from the list and nothing else. If no specific topic matches, return "General Inquiry".


    Topic List: {topics}
    User Query: "{query}"


    Most Relevant Topic:
    """


    prompt = PromptTemplate.from_template(router_prompt_template)
    chain = LLMChain(llm=llm, prompt=prompt)


    try:
        response = chain.invoke({'query': user_query, 'topics': ', '.join(topic_list)})
        topic = response['text'].strip()
        print(f"--- Router Identified Topic: {topic} ---")
        return topic
    except Exception as e:
        print(f"Error during query routing: {e}")
        return "General Inquiry" # Fallback



def get_final_decision_chain(llm):
    """Stage 2: The 'Executor' LLM. Makes the final decision based on filtered context."""
    prompt_template = """
    You are an expert insurance claims analyst. Your task is to make a decision on a user's claim based *only* on the provided policy clauses.


    CONTEXT CLAUSES:
    ---
    {context}
    ---


    USER'S QUERY:
    ---
    {query}
    ---


    INSTRUCTIONS:
    1.  Thoroughly analyze the user's query and the provided context clauses.
    2.  Determine a final decision from one of these four options: "Approved", "Rejected", "Approved with Sub-limit", "Insufficient Information".
    3.  Use "Insufficient Information" ONLY if the provided clauses do not contain the necessary information.
    4.  The "amount" should be the specific amount payable (e.g., "₹25,000") or null if not applicable.
    5.  The "justification" must be a clear explanation referencing the specific rules from the context.


    Provide your response as a single, clean JSON object.


    Final JSON Response:
    """
    rag_chain = LLMChain(
        llm=llm,
        prompt=PromptTemplate.from_template(prompt_template)
    )
    return rag_chain


# --- Automatic Cleanup of Old Documents ---
print("--- Checking for old data files... ---")
patterns = ["*.pdf", "*.docx", "*.doc", "*.eml", "*.msg"]

files_to_delete = []; [files_to_delete.extend(glob.glob(p)) for p in patterns]
if files_to_delete:
    for file_path in files_to_delete: os.remove(file_path)
    print(f"Cleaned up {len(files_to_delete)} old file(s).")
else:
    print("Environment is clean.")

# --- Document Upload and Processing ---
print("\n" + "="*50)
print("Please upload one or more policy documents for the API to use.")
uploaded = files.upload()

vector_store = None
if not uploaded:
    print("\nNo files uploaded. API will not be able to answer questions.")
else:
    file_names = list(uploaded.keys())
    vector_store = create_vector_store_from_paths_universal(file_names)

# --- Initialize Global Variables for the API ---
if vector_store:
    llm = ChatGoogleGenerativeAI(model=MODEL_CONFIG["llm"], temperature=0.0)
    final_decision_chain = get_final_decision_chain(llm)
    all_doc_chunks = list(vector_store.docstore._dict.values())
    print("\n✅ System is ready. Starting API server...")
else:
    print("\n❌ System setup failed. API will not work correctly.")


#6. FLASK API SERVER

app = Flask(__name__)
CORS(app)

@app.route('/predict', methods=['POST'])
def predict():
    if not vector_store:
        return jsonify({"error": "Vector store not initialized. Please upload documents and restart."}), 500

    data = request.get_json()
    user_query = data.get('query')
    if not user_query:
        return jsonify({"error": "Query not provided"}), 400

    print(f"Received API query: {user_query}")
    try:
        # --- This is the Two-Stage Pipeline Logic for the API ---
        topic = get_query_topic_with_llm(user_query, llm)

        if topic != "General Inquiry":
            keyword = topic.split(" ")[0].lower()
            relevant_docs = [doc for doc in all_doc_chunks if keyword in doc.page_content.lower()]
            if not relevant_docs:
                relevant_docs = vector_store.similarity_search(user_query, k=8)
        else:
            relevant_docs = vector_store.similarity_search(user_query, k=8)

        if not relevant_docs:
            return jsonify({"decision": "Insufficient Information", "justification": "No relevant policy clauses could be found."})

        unique_docs = list({doc.page_content: doc for doc in relevant_docs}.values())
        context = "\n\n".join([f"Clause from {doc.metadata.get('source', 'document')}:\n{doc.page_content}" for doc in unique_docs])

        result = final_decision_chain.invoke({"context": context, "query": user_query})

        match = re.search(r'\{.*\}', result.get('text', ''), re.DOTALL)
        if match:
            return jsonify(json.loads(match.group(0)))
        else:
            return jsonify({"decision": "Error", "justification": "Failed to parse JSON from AI."}), 500

    except Exception as e:
        return jsonify({"error": f"An internal error occurred: {e}"}), 500

# --- Ngrok Tunnel Setup ---
NGROK_AUTH_TOKEN = "30mo77JYAmj7yV7asRCNdwcyrjl_3ZT8FhzVaUVea6NMRppWw" # Get token from https://dashboard.ngrok.com/
ngrok.set_auth_token(NGROK_AUTH_TOKEN)

# Start the Flask app in a separate thread
threading.Thread(target=lambda: app.run(port=5000, use_reloader=False)).start()

# Create a public URL to the Flask app
public_url = ngrok.connect(5000)
print("\n" + "="*50)
print("✅ Your API is now live!")
print(f"Public URL: {public_url}")
print(f"Your frontend team should send POST requests to: {public_url}/predict")
print("="*50)