# -*- coding: utf-8 -*-
"""Copy of Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GXVJ0T2Ha7oJkTpIsMc5P3KFaBE_U__C
"""


import os
import json
import re
import tempfile
import base64
import uuid # To create unique session IDs
from flask import Flask, request, jsonify
from flask_cors import CORS, cross_origin
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_unstructured import UnstructuredLoader
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
import glob

if not os.environ.get("GOOGLE_API_KEY"):
    print("ERROR: GOOGLE_API_KEY environment variable not set.")

MODEL_CONFIG = {"llm": "gemini-1.5-flash-latest", "embedding": "models/embedding-001"}



PERSONALITIES = {
    "insurance": {
        "name": "Insurance Claims Analyst",
        "topic_list": [
             "Cataract Surgery Sub-Limit",
        "Maternity Expenses Waiting Period",
        "Maternity Coverage Amount",
        "AYUSH Treatment Coverage",
        "Adventure Sports Exclusion",
        "Hernia Waiting Period",
        "Domiciliary Hospitalization Exclusion",
        "Grace Period Policy",
        "Dental Treatment Accident",
        "Robotic Surgery Sub-Limit",
        "Post-Hospitalization Time Limit",
        "General Inquiry"
        "Knee Surgery",
             "General Enquiry"
        ],
        "prompt_template": """
       You are an expert insurance claims analyst. Your task is to make a decision on a user's claim based *only* on the provided policy clauses.


    CONTEXT CLAUSES:
    ---
    {context}
    ---


    USER'S QUERY:
    ---
    {query}
    ---


    INSTRUCTIONS:
    1.  Thoroughly analyze the user's query and the provided context clauses.
    2.  Determine a final decision from one of these four options: "Approved", "Rejected", "Approved with Sub-limit", "Insufficient Information".
    3.  Use "Insufficient Information" ONLY if the provided clauses do not contain the necessary information.
    4.  The "amount" should be the specific amount payable (e.g., "₹25,000") or null if not applicable.
    5.  The "justification" must be a clear explanation referencing the specific rules from the context.


    Provide your response as a single, clean JSON object.


    Final JSON Response:
        """
    },
    "legal": {
        "name": "Legal Document Reviewer",
        "topic_list": [
            "Agreement Term and Duration",
            "Termination Conditions",
            "Payment and Fees",
            "Liability and Indemnification",
            "Confidentiality Obligations",
            "Intellectual Property Rights",
            "Governing Law and Jurisdiction",
            "Dispute Resolution and Arbitration",
            "Force Majeure",
            "General Inquiry"
        ],
        "prompt_template": """
        You are a professional paralegal assistant. Your task is to review a user's query against the provided legal document clauses.
        CONTEXT: {context}
        USER'S QUERY: {query}
        INSTRUCTIONS:
        1. Analyze the query and the provided legal clauses.
        2. Provide a direct answer to the user's question.
        3. Extract the specific clauses from the context that support your answer.
        Provide your response as a single, clean JSON object with the keys "answer" and "source_clauses".
        Final JSON Response:
        """
    },

    "hr": {
        "name": "HR Policy Advisor",
        "topic_list": [
            "Leave Policy (Vacation, Sick, Funeral)",
            "Work From Home and Remote Work",
            "Expense Reimbursement",
            "Code of Conduct and Harassment",
            "Compensation and Salary",
            "IT and Computer Usage Policy",
            "Hiring and Recruitment",
            "Termination Policy",
            "Employee Benefits",
            "General Inquiry"
        ],
        "prompt_template": """
        You are a helpful HR Policy Advisor. Your task is to answer an employee's question based ONLY on the provided HR policy document excerpts.
        CONTEXT: {context}
        USER'S QUERY: {query}
        INSTRUCTIONS:
        1. Find the relevant rule in the HR policy context.
        2. Provide a clear, direct answer to the employee's question.
        3. Quote the specific policy rule that your answer is based on.
        Provide your response as a single, clean JSON object with the keys "answer" and "relevant_policy_statement".
        Final JSON Response:
        """
    },

   "contract_management": {
    "name": "Contract Analyst",
    "topic_list": [
        "Confidentiality Obligation",
        "Agreement Term and Survival Clause",
        "Termination Procedure",
        "Payment Terms",
        "Intellectual Property (IP)",
        "Liability and Indemnification",
        "Governing Law",
        "General Inquiry"
    ],
    "prompt_template": """
    You are a meticulous Contract Analyst AI. Your task is to review a user's question against the provided contract excerpts and extract the key details.
    CONTEXT: {context}
    USER'S QUERY: {query}
    INSTRUCTIONS:
    1. Find the exact clause that answers the user's question.
    2. Provide a concise summary of that clause.
    3. Quote the full, original text of the clause as the source.
    Provide your response as a single, clean JSON object with the keys "summary_of_clause" and "source_text".
    Final JSON Response:
    """
}
}



def create_vector_store(file_paths):
    # ... (Your existing create_vector_store function)
    all_documents = []

    try:
        for file_path in file_paths:
            loader = UnstructuredLoader(file_path)
            all_documents.extend(loader.load())
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        docs = text_splitter.split_documents(all_documents)
        embeddings = GoogleGenerativeAIEmbeddings(model=MODEL_CONFIG["embedding"])
        vector_store = FAISS.from_documents(docs, embeddings)
        print(f"\nVector store created successfully.")
        return vector_store
    except Exception as e:
        print(f"Error creating vector store: {e}")
        return None

def get_query_topic_with_llm(user_query, llm, topic_list):
    # ... (Your existing get_query_topic_with_llm function)
    router_prompt_template = """
    You are an intelligent router AI. Your task is to analyze the user's query and identify the single most relevant insurance topic from the provided list.
    Return only the topic name from the list and nothing else. If no specific topic matches, return "General Inquiry".
    Topic List: {topics}
    User Query: "{query}"
    Most Relevant Topic:
    """
    prompt = PromptTemplate.from_template(router_prompt_template)
    chain = LLMChain(llm=llm, prompt=prompt)
    try:
        response = chain.invoke({'query': user_query, 'topics': ', '.join(topic_list)})
        topic = response['text'].strip()
        print(f"--- Router Identified Topic: {topic} ---")
        return topic
    except Exception as e:
        print(f"Error during query routing: {e}")
        return "General Inquiry"


def get_final_decision_chain(llm, prompt_template): # Now takes prompt_template as an argument
    """Stage 2: The 'Executor' LLM. Makes the final decision based on filtered context."""
    rag_chain = LLMChain(
        llm=llm,
        prompt=PromptTemplate.from_template(prompt_template)
    )
    return rag_chain



# --- Automatic Cleanup of Old Documents ---
print("--- Checking for old data files... ---")
patterns = ["*.pdf", "*.docx", "*.doc", "*.eml", "*.msg"]

files_to_delete = []; [files_to_delete.extend(glob.glob(p)) for p in patterns]
if files_to_delete:
    for file_path in files_to_delete: os.remove(file_path)
    print(f"Cleaned up {len(files_to_delete)} old file(s).")
else:
    print("Environment is clean.")

# --- Document Upload and Processing ---
# --- Domain Selection Menu ---
app = Flask(__name__)
# Explicitly allow localhost for testing
CORS(app, resources={r"/*": {"origins": "http://localhost:3000"}})

llm = ChatGoogleGenerativeAI(model=MODEL_CONFIG["llm"], temperature=0.0)
rag_chains = {domain: get_final_decision_chain(llm, p["prompt_template"]) for domain, p in PERSONALITIES.items()}
print("✅ AI Models and Chains Initialized Successfully.")

knowledge_bases = {}


@app.route('/')
def home():
    return "Server is live and running!", 200
CORS(app, resources={r"/*": {"origins": "http://localhost:3000"}})

@app.route('/upload', methods=['POST', 'OPTIONS'])
@cross_origin(origins="http://localhost:3000")
def upload_documents():
    try:
        if request.method == 'OPTIONS':
            return '', 200
        data = request.get_json(force=True)
        domain = data.get('domain')
        documents_b64 = data.get('documents')

        if not all([domain, documents_b64]) or not isinstance(documents_b64, list):
            return jsonify({"error": "Request must include 'domain' and a list of 'documents'"}), 400
        session_id = str(uuid.uuid4())
        with tempfile.TemporaryDirectory() as temp_dir:
            file_paths = []
            for i, doc_b64 in enumerate(documents_b64):
                if not isinstance(doc_b64, str):
                    return jsonify({"error": f"Invalid document format at index {i}. Expected a base64 string."}), 400
                file_content = base64.b64decode(doc_b64)
                temp_file_path = os.path.join(temp_dir, f"doc_{i}")
                with open(temp_file_path, "wb") as f:
                    f.write(file_content)
                file_paths.append(temp_file_path)

            vector_store = create_vector_store(file_paths)
            if not vector_store:
                print("Error: create_vector_store returned None.")
                return jsonify({"error": "Failed to process the uploaded documents."}), 500
            knowledge_bases[session_id] = {
                "vector_store": vector_store,
                "all_doc_chunks": list(vector_store.docstore._dict.values())
            }
            print(f"Successfully created knowledge base for session: {session_id}")
            return jsonify({"session_id": session_id, "message": "Documents processed successfully."})
    except Exception as e:
        print(f"An internal error occurred during upload: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({"error": f"An internal error occurred during upload: {str(e)}"}), 500

@app.route('/query', methods=['POST'])
def handle_query():
    data = request.get_json()
    user_query = data.get('query')
    domain = data.get('domain')
    session_id = data.get('session_id')
    if not all([user_query, domain, session_id]):
        return jsonify({"error": "Request must include 'query', 'domain', and 'session_id'"}), 400
    if session_id not in knowledge_bases:
        return jsonify({"error": "Invalid session_id. Please upload documents first."}), 400
    try:
        kb = knowledge_bases[session_id]
        personality = PERSONALITIES[domain]
        final_decision_chain = rag_chains[domain]
        topic = get_query_topic_with_llm(user_query, llm, personality["topic_list"])
        relevant_docs = kb["vector_store"].similarity_search(user_query, k=8)
        unique_docs = list({doc.page_content: doc for doc in relevant_docs}.values())
        context = "\n\n".join([f"Clause from document:\n{doc.page_content}" for doc in unique_docs])
        result = final_decision_chain.invoke({"context": context, "query": user_query})
        match = re.search(r'\{.*\}', result.get('text', ''), re.DOTALL)
        if match:
            return jsonify(json.loads(match.group(0)))
        else:
            return jsonify({"error": "Failed to parse JSON from AI."}), 500
    except Exception as e:
        return jsonify({"error": f"An internal error occurred during query: {str(e)}"}), 500
